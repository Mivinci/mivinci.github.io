<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>被门夹过的核桃还补脑吗</title><link>https://xjj.pub/</link><description>Recent content on 被门夹过的核桃还补脑吗</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© XJJ</copyright><lastBuildDate>Thu, 05 Aug 2021 13:53:48 +0800</lastBuildDate><atom:link href="https://xjj.pub/index.xml" rel="self" type="application/rss+xml"/><item><title>OAuth2.0 中的 state 参数</title><link>https://xjj.pub/state_param_in_oauth2/</link><pubDate>Thu, 05 Aug 2021 13:53:48 +0800</pubDate><guid>https://xjj.pub/state_param_in_oauth2/</guid><description>OAuth2.0 协议用于用户授权第三方应用，使之能访问受保护的用户资源。RFC67491 中规定了 OAuth2.0 的 4 种授权方式，分别是：
授权码式（authorization code） 隐含式（implicit） 密码式（resource owner password credentials） 客户端模式（client credentials） 且整个协议过程由 4 个部分组成，分别是：
客户端（client） 资源拥有者（resource owner） 授权服务器（authorization server） 资源服务器（resource server） 前两种授权方式需要客户端在访问授权服务器的 authorize 接口时带上一个 state 参数，用来防御跨站请求伪造（CSRF）攻击。
以授权码方式为例，该方式要求客户端提供一个回调地址（redirection uri）用来接受来自授权服务器带有授权码（code）的重定向请求。即该回调地址是对外开放的，客户端为了确定该重定向的确来自授权服务器，而不是其他由攻击者伪造的请求，就在获取授权码阶段，将一个 state 字段通过 url 查询参数（querystring）告诉给授权服务器，授权服务器在重定向时原封不动的也通过 url 查询参数返回该 state 给客户端，然后客户端通过校验 state 是否正确来判断该重定向请求是否来自刚刚请求的授权服务器，从而达到对 CSRF 的防御。
所以，state 参数的生成和校验需要由客户端妥善处理，且参数值要让攻击者难以在有限时间内试出。在完成一次授权过程后，需要立即销毁该 state 参数，保证每次使用不同的参数值。
https://www.rfc-editor.org/rfc/rfc6749.html&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>我的博客搭建历程</title><link>https://xjj.pub/blog-building-experience/</link><pubDate>Tue, 20 Jul 2021 23:35:21 +0800</pubDate><guid>https://xjj.pub/blog-building-experience/</guid><description>和大多数人一样，我的博客历程从 Hexo 的 NexT 主题开始，并且也经历了从折腾各种主题、各种花里胡哨的功能到极简主义这种寻找所谓「真理」的过程，hhh 😁 。不过和大佬们不一样，我基本没写过什么博客文章，一来是觉得自己对熟悉的东西研究得不够深入，二来就是大多数人在维护博客过程中遇到的问题，我愿称其为学习过程中的「贤者模式」。意思是当学会某个东西的那瞬间，之前决心写篇文章的冲动荡然无存，甚至觉得这么简单的东西写出来不是笑话吗。
不过，我想这种学习过程的贤者时间的产生可能也是因为第一个原因。学会了，但没完全学会。特别是在计算机软件领域，在这个满地框架和各种使用示例的时代，学习成本实在太低，学会使用某个工具或掌握某项技术或许并不要求过多去研究这东西背后的原理。所以，或许当人研究深度的提升会逐渐抵消这种贤者模式的影响。
话题掰回来，第一次使用 Hexo 大概是在 2018 年的暑假，也是我暂停服务端开发转而学习 web 前端的开端，但不久就也对很多人诟病的 node_modules 产生了反感，甚至反胃，也许这就是传说中的「晕包」hhh 😅 。这也是我第一次开始怀疑 web 前端是否在朝正确的方向发展。
于是几个月后，和部分使用过 Hexo 的人一样，开始使用 Hugo，这也是一个静态博客生成引擎，不过它的官网号称世界最快。也许是最快吧（若不考虑 Rust 生态的 mdbook 的话，不过 mdbook 面相的是文档快速生成，并非个人博客，所以两者并不形成竞争关系）但肯定的是 Hugo 比 Hexo 快得多，并且没有庞大的包依赖文件。这得益于 Hugo 的开发语言：Go，这也是我第二喜欢的编程语言（😁 第一当然是 C）
直到现在，本站仍然使用的是 Hugo，主题是使用的我自己开发的 Minima，有兴趣的小伙伴可以试试，顺便三连点个 star。
其中，中途还接触过动态博客，如 wordpress，typecho，甚至还自己开发过博客引擎，最后都因为安全问题夭折。动态博客比起 Hexo 或 Hugo 这样的好处我想就是方便发布和管理文章吧，因为动态博客往往都自带有后台管理系统，这也是极易出现安全问题的方面。
经历了这些后，现在觉得极简才是真理，一个优秀的博客系统需要拥有的特性无非就是：
良好的 SEO 方便的文章管理机制 安全性 其中第一项 SEO（搜索引擎优化）不论是静态还是动态，现在都可以配置到很好的效果，因为从原理上讲，动态博客通常是由服务端现获取数据来返回的 HTML 内容，而静态则是提前将数据渲染成 HTML 文件再交给托管服务托管。所以 SSR 效果两者大同小异，剩下的就看搜索引擎给不给力了。另外在分布式时代，博客响应性能也大部分交给 CDN 平台决定，和博客系统本身关系不大。
第二项是动态博客系统的优势，对于静态博客，会稍微麻烦一点。因为其博客构建阶段在本地完成，所以每次想要更新就不得不打开电脑运行命令来重新生成整个博客网页资源，再使用命令发布到托管平台。但其实可以利用第三方 CI/CD 平台，如 travis，再结合代码托管平台，如 GitHub，来完成自动构建+部署的功能，作者就只需要将注意力集中在文章撰写上。本博客站就使用的是 GitHub 自带的 Actions 来实现的自动构建和部署（具体方法下次再补上吧，下次一定！），总之我每次只需要将文章提交到 GitHub，它便会自动为我构建好网页代码资源，然后将其部署到我的个人域名 xjj.</description></item><item><title>Hi, there 👋</title><link>https://xjj.pub/about/</link><pubDate>Sun, 18 Jul 2021 11:24:06 +0800</pubDate><guid>https://xjj.pub/about/</guid><description>关于我 我是 XJJ，不是洗洁精也没有想静静，你可能是通过我的网名「被门夹过的核桃还补脑吗」认识我的 😁 。我今年(2021)毕业于重庆某大学某物联网专业，接下来的三年不出意外会在这儿当炼丹师 🤮
这是我第三次开始写博客，之前两次没能坚持下去可能都是因为使用了别人的博客主题(借口)，所以这次索性先写了个主题再搭博客。目前已经可以在 Hugo 主题官网 上下载这款叫 Minima 的主题。同时，该主题集成了我写的一个评论插件 OvO。和 Waline 一样，OvO 可以嵌入在其他网页上，为其提供评论功能。
为什么非要写博客？诶，就是玩儿！🤣
友链 以下是臭弟弟们的个人站点，若想加入我们，可以联系我(QQ 1366723936) 🤺
昵称 网址 盒子妹妹 is.boxmoe.cn 学霸弟弟 aye.ink 瘠薄弟弟 sang.pub Dejavu dejavu.moe Rust 之神 blog.nmslwsnd.com 御网尚书 hack-gov.com.cn 若梦小窝 rrrmr.cn 萎靡鸭 hack-er.cn 挖个坑，后面写个友链插件美化一下吧&amp;hellip;</description></item><item><title>实现了个 brainfuck 解释器</title><link>https://xjj.pub/brainfuck_interpretor/</link><pubDate>Fri, 02 Jul 2021 14:38:08 +0800</pubDate><guid>https://xjj.pub/brainfuck_interpretor/</guid><description>安装 首先 clone 这个仓库或直接 点击这 下载源码，然后使用下面命令编译安装。
1 2 make make install 语法 Brainfuck 运行在一组内存单元格上，每个格子的初始值为零，并只使用一个指针来操作这些单元格，具体语法如下表所示。
指令 作用 &amp;gt; 将指针右移一格 &amp;lt; 将指针左移一格 + 将指针当前所指单元格的值加1 - 将指针当前所指单元格的值减1 . 打印指针当前所指单元格的值对应的ACSCII字符 , 从键盘获取一个字符存入指针当前所指单元格 [ 若指针当前所指单元格的值为0，则跳转到与之对应的]处继续执行 ] 若指针当前所指单元格的值不为0，则跳回与之对应的[处继续执行 可以看出，brainfuck 只有一个指针、输入输出和循环功能，但它是 图灵完备 的，可以实现和图灵机等同的功能。esolangs.org 上还收集了许多基于 brainfuck 的另类编程语言。
Hello world 老规矩，认识一门语言从打印 Hello World!</description></item><item><title>万物皆弹簧</title><link>https://xjj.pub/tayor-expansion/</link><pubDate>Mon, 03 Feb 2020 18:34:00 +0800</pubDate><guid>https://xjj.pub/tayor-expansion/</guid><description>你要是上过幼儿园小班呢，应该知道势能是位置的函数，表示为 $V(\textbf{r})$，其中 $\textbf{r}$是位矢。在一范围内若某个点的势能达到极小值，则称该点为稳态点 $\textbf{r}_s$，即
对势能函数在稳态点使用泰勒展开并忽略 2 阶后的项得到
$$ V(\textbf{r})=V(\textbf{r}_s)+V^{'}(\textbf{r}_s)(\textbf{r}-\textbf{r}_s)+\frac{V^{''}(\textbf{r}_s)}{2!}(\textbf{r}-\textbf{r}_s)^2 $$
然后把稳态点当作零势能参考点，则有
$$ V(\textbf{r}_s)=0 $$
联立这三个式子得到稳态点处势能的低阶近似为
$$ V(\textbf{r})\approx\frac{V^{''}(\textbf{r}_s)}{2!}(\textbf{r}-\textbf{r}_s)^2 $$
这是势能关于位置的二次函数，其中 $V^{''}(\textbf{r}_s)$ 为某个常数 $k$。
到这儿，要是你还有幸上过幼儿园大班的话，应该知道「力」是势能对位置的导数，所以稳态点附近的粒子受到的力为
$$ \textbf{F}(\textbf{r})=-\frac{dV(\textbf{r})}{d\textbf{r}}=-k(\textbf{r}-\textbf{r}_s) $$
其中 $\textbf{r}-\textbf{r}_s$ 表示在稳态点附近的位移变化，用 $\Delta{\textbf{r}}$ 表示吧。
若是一维的情况，上式变为
$$ \textbf{F}(\textbf{x})=-k\Delta{\textbf{x}} $$
这不就是「胡克定律」吗，即弹簧的弹力与其位移变化量成正比。其中的负号表示该力为吸引力。
这告诉我们在稳态点附近的受力情况可以不依赖势能函数的具体形式，只跟位置变化量成正比关系，近似一种弹簧的弹力。
出现在稳态点附近的运动无处不在，小到固体中原子因与附近原子相互作用而产生的震动，大到你这几天躺床上强行被叫起来吃饭时心里产生的扰动，都可以看成是弹力的作用。所以人类的本质其实是弹簧，你越处在使自己舒适的地方，你就越靠近你的稳态点，你受力的低阶近似就越精确，你就越是根弹簧。</description></item><item><title>关于“信息量大”这句话</title><link>https://xjj.pub/entropy/</link><pubDate>Fri, 03 Jan 2020 19:40:53 +0800</pubDate><guid>https://xjj.pub/entropy/</guid><description>前几天在知乎上看到一个问题：一句话的信息量能大到什么程度？看到评论区很多回复像
对方正在输入&amp;hellip; 一切皆有可能 呵呵…… 之类的，还获得了很多的点赞。当时突然想什么是信息，什么又是信息量？当你面对一件事情，难道不是那些有用的数据对你来说才是信息吗，其他的干扰应该是噪声啊。比如你和一个人聊天，对方给你的回复才是信息呀，而显示 “对方正在输入&amp;hellip;”，并不会让你知道对方要说什么，这句话提供的信息仅仅是“对方正在干回复这件事儿”，而对方要回复的内容是什么，还是有很大的不确定性，这个不确定性是用信息熵来衡量的。
所以我们一直在把信息和信息熵混用。像上面三句话其实是信息熵很大，而信息量反而是很小的，因为你并不能从这些话里的出什么确切的结论。
”一切皆有可能“ 这句话更是直接告诉了你信息熵相当的大。为什么一切皆有可能？是因为你还没有得到足够多的信息来减小事情的不确定性。举个最简单的例子，你的沙雕同学抢了你的硬币，并让你猜在他哪只手里，在他告诉你答案之前，你对硬币在他哪只手是不确定的，现在他告诉你他的左手没有硬币，若他没有说谎，那么他就给了你信息，让你对硬币在他哪只手的不确定性从1降到了2/3，也就是说你的沙雕同学给你了1/3的信息，这1/3的信息通过声波传给了你。
故对于任何事情，你得通过各种方法去收集信息来消灭不确定性，减小信息熵。所以今晚吃什么 😵</description></item><item><title>怎么和最喜欢的人在一起？</title><link>https://xjj.pub/e/</link><pubDate>Mon, 09 Dec 2019 01:40:53 +0800</pubDate><guid>https://xjj.pub/e/</guid><description>你一生中会遇到很多的人，那么如何用最少的交往次数，找到最好的那个和他在一起呢？当然你说你就要全部试一遍，那我也阻止不了你。
首先，你可以先试着和几个人在一起(滑稽)，一段时间后，不管他们有多好，都甩了他们。然后接下来遇到的人，只要比前面被你甩的任何一个人好，就和他一起私奔。好了现在来把问题公式化一下(哎～别走呀，听我说完嘛)
假设有 n 个人，你要放弃前 k 个，从第 k+1 个开始，遇到比前 k 个好的就和他在一起，求 k 为多少使得最好的人被你选中的概率最大。
我们设
事件 $A$ 为：最好的人被你选中
事件 $B_i$ 为：第 i 个人是最好的
我们有全概率公式 $$ P(A)=\sum_{i=k+1}^{n}{P(B_i)P(A|B_i)} $$
其中，这 n 个人是均匀分布的，$P(B_i)$ 就为 $\frac{1}{n}$，所以只要求出 $P(A|B_i)$，就能算出最好的人被你选到的概率 $P(A)$ ，然后再使 $P(A)$ 的导数等于零，就能解出 k 了。
而 $P(A|B_i)$ 表示在第 i 个人是最好的那个人的条件下，最好的被你选到的概率。也就是说如果最好的人被你选到，那第 i-1 个就没有前 k 个好，而第 i-1 个没有前 k 个好的概率为 $\frac{k}{i-1}$，所以
$$ P(A)=\frac{1}{n}\sum_{i=k+1}^{n}{\frac{k}{i-1}}=\frac{k}{n}\sum_{i=k+1}^{n-1}{\frac{1}{i}} $$
假设 n 很大很大，也就是你会遇到很多很多的人，我们令 $x=\frac{k}{n}, f(t)=\frac{1}{t}$，上式可以转换为定积分
求 $P(A)$ 极值，令
得到 $x=\frac{1}{e}$ ，所以 $k=\frac{n}{e} $
可以看一下是否是极大值，令</description></item><item><title>朴素贝叶斯分类器</title><link>https://xjj.pub/bayes/</link><pubDate>Sun, 10 Nov 2019 13:52:53 +0800</pubDate><guid>https://xjj.pub/bayes/</guid><description>这篇文章将对朴素贝叶斯分类器进行简单描述和公式推导，并以性别预测为例来理解其「朴素」的原因，最后你应该能根据文章的描述编写一个简单的由人的姓名预测其性别的小程序。阅读前你可能需要懂一些概率论的知识，比如条件概率、贝叶斯公式等等。
基本原理 我们在小学二年级学过贝叶斯公式：
$$ P(B|A)=\frac{P(B)P(A|B)}{P(A)} \tag{1} $$
通过该式子，我们可以算出在已知事件A发生的条件下，事件B发生的概率。比如1当我们看到室友抽屉里藏女士内衣，则室友是个变态的概率就是遇到变态室友的概率乘室友是个变态还喜欢把内衣放抽屉里的概率再除以室友抽屉里有内衣的概率。是不是非常的啊妹zing呀？这里面其实蕴含着「贝叶斯学派」独有的思想，即「后验概率」的思想。感兴趣的同学可以深入了解一下。
现在我们假设上式中的事件A由很多事件构成，即：
$$ A=A_1 \cap A_2 \cap &amp;hellip; \cap A_n $$
则我们的贝叶斯公式变成：
$$ P(B|A_1 A_2 &amp;hellip; A_n)=\frac{P(B)P(A_1 A_2 &amp;hellip; A_n|B)}{P(A_1 A_2 &amp;hellip; A_n)} \tag{2} $$
这其实已经是一个分类器的模型了。我们不妨将事件B看成一推数据的分类结果，将 $A_1,A_2,&amp;hellip;,A_n$ 看成是导致该结果的因素或者叫特征（以下统称特征），则等号左边表示的就是我们的数据属于分类 $B$ 的概率，当其达到某个阈值，我们可以认为该推数据就属于某个分类。
要得到等号左边是多少，我们得从已有的数据中找到等号右边的三项的值。其中 $P(B)$ 可以通过统计数据集中 $B$ 类样本出现的频次得到，即：
$$ P(B)=\frac{B类样本个数}{样本总个数} $$
但剩下的两项很难从有限的数据中直接得出。对于 $P(A_1 A_2 &amp;hellip; A_n|B)$，我们假设所有的特征相互独立，则我们就有： 2
$$ P(A_1 A_2 &amp;hellip; A_n|B)=\prod_{i=1}^{n}{P(A_i|B)} \tag{3} $$
其中 $P(A_i|B)$ 是容易从数据集中直接得到的，分为离散和连续两种情况：
当 $A_i$ 为离散型随机变量时 $$ P(A_i|B)=\frac{B类中有特征A_i的数据个数}{B类数据个数} $$
当 $A_i$ 为连续型随机变量时 假设 $A_i|B$ 符合期望为 $\mu_{ki}$，方差为 $\sigma^2_{ki}$ 的正态分布，则有：</description></item></channel></rss>