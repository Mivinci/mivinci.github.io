<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>默认分组 on 被门夹过的核桃还补脑吗</title><link>https://xjj.pub/categories/%E9%BB%98%E8%AE%A4%E5%88%86%E7%BB%84/</link><description>Recent content in 默认分组 on 被门夹过的核桃还补脑吗</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© XJJ 2021</copyright><lastBuildDate>Sun, 18 Jul 2021 11:24:06 +0800</lastBuildDate><atom:link href="https://xjj.pub/categories/%E9%BB%98%E8%AE%A4%E5%88%86%E7%BB%84/index.xml" rel="self" type="application/rss+xml"/><item><title>哈喽 👋</title><link>https://xjj.pub/about/</link><pubDate>Sun, 18 Jul 2021 11:24:06 +0800</pubDate><guid>https://xjj.pub/about/</guid><description>我是 XJJ，不是洗洁精也没有想静静，你可能是通过我的网名「被门夹过的核桃还补脑吗」认识我的。我同时也是：
理想主义者 理论物理学爱好者 会点技术 义务教育漏网之鱼 这是我第三次开始写博客，之前两次没能坚持下去可能都是因为使用了别人的博客主题(借口)，所以这次索性先写了个主题再搭博客。目前已经可以在 Hugo 主题官网 上下载这款叫 Minima 的主题。同时，该主题集成了我写的一个评论插件 OvO。和 Waline 一样，OvO 可以嵌入在其他网页上，为其提供评论功能。
为什么非要写博客？诶，就是玩儿！🤣
haha, 言归正传。为什么还是想写博客？引用句触发心声的话：
我不是程序员，也不是设计师，我只是恰好有一些想法和一台电脑。
没错，就是平时脑子里充斥着很多想法，无论是技术上的还是生活上的，也无论是熟悉的领域的还是不熟悉的领域的，都会时不时涌现出一些想法。有的会驱动着我去学习，有的会使我去实现，还有的会让我反感。时间久了，有些想法还会重复出现。所以，当我尝试着将这些想法记录下来后，会有一种解脱的感觉，就像是脑子里有个任务队列，队头等着被执行。
不过，由于平时并没有较多的精力来记录，所以你会看到前几年只有稀稀散散几篇文章，不过我想今后应该会有所改善，并尽量多记录下学习上，特别是对基础理论知识的总结，像统计学、信息论、计算理论等等。我个人并不太喜欢记录技术上的东西，除了安利自己的小项目 haha，因为对于技术，我一直很反感为了钻研而钻研这种花大量时间收益却很少的学习方式。除非遇到对某个技术原理的需求的时候，不然不会在那些细枝末节上逗留很久。这可能仅仅是我对技术的偏见吧。
同时，写博客其实是给你的学习和生活设置检查点或存档点，日后翻看自己写的文章时，能够通过这些文字激发重新学习相关内容的动力和机会，因为你也许会忘记曾经几时见到学到些什么。这也就是为什么我一直建议不要把经别人压缩提炼后的知识当做自己的知识，而是应该通过这些提炼物，找到相关书籍文献进行系统地学习。应该将其当做激发学习兴趣的工具。
另外，可能有些文章中提到的想法比较激进，那就当我是在圈地自萌，也欢迎跟我对线。
友链 以下是臭弟弟们的个人站点，换友联可联系我 (QQ 1366723936) 🤺
昵称 网址 盒子妹妹 is.boxmoe.cn 学霸弟弟 aye.ink 瘠薄弟弟 sang.pub Dejavu dejavu.moe Rust 之神 blog.nmslwsnd.com 御网尚书 hack-gov.com.cn 若梦小窝 rrrmr.cn 萎靡鸭 hack-er.</description></item><item><title>万物皆弹簧</title><link>https://xjj.pub/idea/tayor-expansion/</link><pubDate>Mon, 03 Feb 2020 18:34:00 +0800</pubDate><guid>https://xjj.pub/idea/tayor-expansion/</guid><description>你要是上过幼儿园小班呢，应该知道势能是位置的函数，表示为 $V(\textbf{r})$，其中 $\textbf{r}$是位矢。在一范围内若某个点的势能达到极小值，则称该点为稳态点 $\textbf{r}_s$，即
对势能函数在稳态点使用泰勒展开并忽略 2 阶后的项得到
$$ V(\textbf{r})=V(\textbf{r}_s)+V^{'}(\textbf{r}_s)(\textbf{r}-\textbf{r}_s)+\frac{V^{''}(\textbf{r}_s)}{2!}(\textbf{r}-\textbf{r}_s)^2 $$
然后把稳态点当作零势能参考点，则有
$$ V(\textbf{r}_s)=0 $$
联立这三个式子得到稳态点处势能的低阶近似为
$$ V(\textbf{r})\approx\frac{V^{''}(\textbf{r}_s)}{2!}(\textbf{r}-\textbf{r}_s)^2 $$
这是势能关于位置的二次函数，其中 $V^{''}(\textbf{r}_s)$ 为某个常数 $k$。
到这儿，要是你还有幸上过幼儿园大班的话，应该知道「力」是势能对位置的导数，所以稳态点附近的粒子受到的力为
$$ \textbf{F}(\textbf{r})=-\frac{dV(\textbf{r})}{d\textbf{r}}=-k(\textbf{r}-\textbf{r}_s) $$
其中 $\textbf{r}-\textbf{r}_s$ 表示在稳态点附近的位移变化，用 $\Delta{\textbf{r}}$ 表示吧。
若是一维的情况，上式变为
$$ \textbf{F}(\textbf{x})=-k\Delta{\textbf{x}} $$
这不就是「胡克定律」吗，即弹簧的弹力与其位移变化量成正比。其中的负号表示该力为吸引力。
这告诉我们在稳态点附近的受力情况可以不依赖势能函数的具体形式，只跟位置变化量成正比关系，近似一种弹簧的弹力。
出现在稳态点附近的运动无处不在，小到固体中原子因与附近原子相互作用而产生的震动，大到你这几天躺床上强行被叫起来吃饭时心里产生的扰动，都可以看成是弹力的作用。所以人类的本质其实是弹簧，你越处在使自己舒适的地方，你就越靠近你的稳态点，你受力的低阶近似就越精确，你就越是根弹簧。</description></item><item><title>关于「信息量大」这句话</title><link>https://xjj.pub/idea/entropy/</link><pubDate>Fri, 03 Jan 2020 19:40:53 +0800</pubDate><guid>https://xjj.pub/idea/entropy/</guid><description>前几天在知乎上看到一个问题：一句话的信息量能大到什么程度？看到评论区很多回复像
对方正在输入&amp;hellip; 一切皆有可能 呵呵…… 之类的，还获得了很多的点赞。当时突然想什么是信息，什么又是信息量？当你面对一件事情，难道不是那些有用的数据对你来说才是信息吗，其他的干扰应该是噪声啊。比如你和一个人聊天，对方给你的回复才是信息呀，而显示 “对方正在输入&amp;hellip;”，并不会让你知道对方要说什么，这句话提供的信息仅仅是“对方正在干回复这件事儿”，而对方要回复的内容是什么，还是有很大的不确定性，这个不确定性是用信息熵来衡量的。
所以我们一直在把信息和信息熵混用。像上面三句话其实是信息熵很大，而信息量反而是很小的，因为你并不能从这些话里的出什么确切的结论。
”一切皆有可能“ 这句话更是直接告诉了你信息熵相当的大。为什么一切皆有可能？是因为你还没有得到足够多的信息来减小事情的不确定性。举个最简单的例子，你的沙雕同学抢了你的硬币，并让你猜在他哪只手里，在他告诉你答案之前，你对硬币在他哪只手是不确定的，现在他告诉你他的左手没有硬币，若他没有说谎，那么他就给了你信息，让你对硬币在他哪只手的不确定性从1降到了2/3，也就是说你的沙雕同学给你了1/3的信息，这1/3的信息通过声波传给了你。
故对于任何事情，你得通过各种方法去收集信息来消灭不确定性，减小信息熵。所以今晚吃什么 😵</description></item><item><title>怎么和最喜欢的人在一起？</title><link>https://xjj.pub/idea/e/</link><pubDate>Mon, 09 Dec 2019 01:40:53 +0800</pubDate><guid>https://xjj.pub/idea/e/</guid><description>你一生中会遇到很多的人，那么如何用最少的交往次数，找到最好的那个和他在一起呢？当然你说你就要全部试一遍，那我也阻止不了你。
首先，你可以先试着和几个人在一起(滑稽)，一段时间后，不管他们有多好，都甩了他们。然后接下来遇到的人，只要比前面被你甩的任何一个人好，就和他一起私奔。好了现在来把问题公式化一下(哎～别走呀，听我说完嘛)
假设有 n 个人，你要放弃前 k 个，从第 k+1 个开始，遇到比前 k 个好的就和他在一起，求 k 为多少使得最好的人被你选中的概率最大。
我们设
事件 $A$ 为：最好的人被你选中
事件 $B_i$ 为：第 i 个人是最好的
我们有全概率公式 $$ P(A)=\sum_{i=k+1}^{n}{P(B_i)P(A|B_i)} $$
其中，这 n 个人是均匀分布的，$P(B_i)$ 就为 $\frac{1}{n}$，所以只要求出 $P(A|B_i)$，就能算出最好的人被你选到的概率 $P(A)$ ，然后再使 $P(A)$ 的导数等于零，就能解出 k 了。
而 $P(A|B_i)$ 表示在第 i 个人是最好的那个人而这个人恰好被你选到的概率。那第 i-1 个就没有前 k 个好，而第 i-1 个没有前 k 个好的概率为 $\frac{k}{i-1}$，所以
$$ P(A)=\frac{1}{n}\sum_{i=k+1}^{n}{\frac{k}{i-1}}=\frac{k}{n}\sum_{i=k+1}^{n-1}{\frac{1}{i}} $$
假设 n 很大很大，也就是你会遇到很多很多的人，我们令 $x=\frac{k}{n}, f(t)=\frac{1}{t}$，上式可以转换为定积分
求 $P(A)$ 极值，令
$$ \frac{dP(A)}{dx}=\frac{d(-xlnx)}{dx}=1-lnx=0 $$
得到 $x=\frac{1}{e}$ ，所以 $k=\frac{n}{e} $</description></item><item><title>朴素贝叶斯分类器</title><link>https://xjj.pub/idea/bayes/</link><pubDate>Sun, 10 Nov 2019 13:52:53 +0800</pubDate><guid>https://xjj.pub/idea/bayes/</guid><description>这篇文章将对朴素贝叶斯分类器进行简单描述和公式推导，并以性别预测为例来理解其「朴素」的原因，最后你应该能根据文章的描述编写一个简单的由人的姓名预测其性别的小程序。阅读前你可能需要懂一些概率论的知识，比如条件概率、贝叶斯公式等等。
基本原理 我们在小学二年级学过贝叶斯公式：
$$ P(B|A)=\frac{P(B)P(A|B)}{P(A)} \tag{1} $$
通过该式子，我们可以算出在已知事件A发生的条件下，事件B发生的概率。比如1当我们看到室友抽屉里藏女士内衣，则室友是个变态的概率就是遇到变态室友的概率乘室友是个变态还喜欢把内衣放抽屉里的概率再除以室友抽屉里有内衣的概率。是不是非常的啊妹zing呀？这里面其实蕴含着「贝叶斯学派」独有的思想，即「后验概率」的思想。感兴趣的同学可以深入了解一下。
现在我们假设上式中的事件A由很多事件构成，即：
$$ A=A_1 \cap A_2 \cap &amp;hellip; \cap A_n $$
则我们的贝叶斯公式变成：
$$ P(B|A_1 A_2 &amp;hellip; A_n)=\frac{P(B)P(A_1 A_2 &amp;hellip; A_n|B)}{P(A_1 A_2 &amp;hellip; A_n)} \tag{2} $$
这其实已经是一个分类器的模型了。我们不妨将事件B看成一推数据的分类结果，将 $A_1,A_2,&amp;hellip;,A_n$ 看成是导致该结果的因素或者叫特征（以下统称特征），则等号左边表示的就是我们的数据属于分类 $B$ 的概率，当其达到某个阈值，我们可以认为该推数据就属于某个分类。
要得到等号左边是多少，我们得从已有的数据中找到等号右边的三项的值。其中 $P(B)$ 可以通过统计数据集中 $B$ 类样本出现的频次得到，即：
$$ P(B)=\frac{B类样本个数}{样本总个数} $$
但剩下的两项很难从有限的数据中直接得出。对于 $P(A_1 A_2 &amp;hellip; A_n|B)$，我们假设所有的特征相互独立，则我们就有： 2
$$ P(A_1 A_2 &amp;hellip; A_n|B)=\prod_{i=1}^{n}{P(A_i|B)} \tag{3} $$
其中 $P(A_i|B)$ 是容易从数据集中直接得到的，分为离散和连续两种情况：
当 $A_i$ 为离散型随机变量时 $$ P(A_i|B)=\frac{B类中有特征A_i的数据个数}{B类数据个数} $$
当 $A_i$ 为连续型随机变量时 假设 $A_i|B$ 符合期望为 $\mu_{ki}$，方差为 $\sigma^2_{ki}$ 的正态分布，则有：</description></item></channel></rss>