<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:title" content="朴素贝叶斯分类器"><meta property="og:description" content="这篇文章将对朴素贝叶斯分类器进行简单描述和公式推导，并以性别预测为例来理解其「朴素」的原因，最后你应该能根据文章的描述编写一个简单的由人的姓名预测其性别的小程序。阅读前你可能需要懂一些概率论的知识，比如条件概率、贝叶斯公式等等。
基本原理 我们在小学二年级学过贝叶斯公式：
$$ P(B|A)=\frac{P(B)P(A|B)}{P(A)} \tag{1} $$
通过该式子，我们可以算出在已知事件A发生的条件下，事件B发生的概率。比如1当我们看到室友抽屉里藏女士内衣，则室友是个变态的概率就是遇到变态室友的概率乘室友是个变态还喜欢把内衣放抽屉里的概率再除以室友抽屉里有内衣的概率。这里面其实蕴含「后验概率」的思想。感兴趣的可以 了解下。
现在我们假设上式中的事件A由很多事件构成，即：
$$ A=A_1 \cap A_2 \cap &mldr; \cap A_n $$
则我们的贝叶斯公式变成：
$$ P(B|A_1 A_2 &mldr; A_n)=\frac{P(B)P(A_1 A_2 &mldr; A_n|B)}{P(A_1 A_2 &mldr; A_n)} \tag{2} $$
这其实已经是一个分类器的模型了。我们不妨将事件B看成一推数据的分类结果，将 $A_1,A_2,&mldr;,A_n$ 看成是导致该结果的因素或者叫特征（以下统称特征），则等号左边表示的就是我们的数据属于分类 $B$ 的概率，当其达到某个阈值，我们可以认为该推数据就属于某个分类。
要得到等号左边是多少，我们得从已有的数据中找到等号右边的三项的值。其中 $P(B)$ 可以通过统计数据集中 $B$ 类样本出现的频次得到，即：
$$ P(B)=\frac{B类样本个数}{样本总个数} $$
但剩下的两项很难从有限的数据中直接得出。对于 $P(A_1 A_2 &mldr; A_n|B)$，我们假设所有的特征相互独立，则我们就有： 2
$$ P(A_1 A_2 &mldr; A_n|B)=\prod_{i=1}^{n}{P(A_i|B)} \tag{3} $$
其中 $P(A_i|B)$ 是容易从数据集中直接得到的，分为离散和连续两种情况：
 当 $A_i$ 为离散型随机变量时  $$ P(A_i|B)=\frac{B类中有特征A_i的数据个数}{B类数据个数} $$"><meta property="og:type" content="article"><meta property="og:url" content="https://xjj.pub/idea/bayes/"><meta property="article:section" content="idea"><meta property="article:published_time" content="2019-11-10T13:52:53+08:00"><meta property="article:modified_time" content="2019-11-10T13:52:53+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="朴素贝叶斯分类器"><meta name=twitter:description content="这篇文章将对朴素贝叶斯分类器进行简单描述和公式推导，并以性别预测为例来理解其「朴素」的原因，最后你应该能根据文章的描述编写一个简单的由人的姓名预测其性别的小程序。阅读前你可能需要懂一些概率论的知识，比如条件概率、贝叶斯公式等等。
基本原理 我们在小学二年级学过贝叶斯公式：
$$ P(B|A)=\frac{P(B)P(A|B)}{P(A)} \tag{1} $$
通过该式子，我们可以算出在已知事件A发生的条件下，事件B发生的概率。比如1当我们看到室友抽屉里藏女士内衣，则室友是个变态的概率就是遇到变态室友的概率乘室友是个变态还喜欢把内衣放抽屉里的概率再除以室友抽屉里有内衣的概率。这里面其实蕴含「后验概率」的思想。感兴趣的可以 了解下。
现在我们假设上式中的事件A由很多事件构成，即：
$$ A=A_1 \cap A_2 \cap &mldr; \cap A_n $$
则我们的贝叶斯公式变成：
$$ P(B|A_1 A_2 &mldr; A_n)=\frac{P(B)P(A_1 A_2 &mldr; A_n|B)}{P(A_1 A_2 &mldr; A_n)} \tag{2} $$
这其实已经是一个分类器的模型了。我们不妨将事件B看成一推数据的分类结果，将 $A_1,A_2,&mldr;,A_n$ 看成是导致该结果的因素或者叫特征（以下统称特征），则等号左边表示的就是我们的数据属于分类 $B$ 的概率，当其达到某个阈值，我们可以认为该推数据就属于某个分类。
要得到等号左边是多少，我们得从已有的数据中找到等号右边的三项的值。其中 $P(B)$ 可以通过统计数据集中 $B$ 类样本出现的频次得到，即：
$$ P(B)=\frac{B类样本个数}{样本总个数} $$
但剩下的两项很难从有限的数据中直接得出。对于 $P(A_1 A_2 &mldr; A_n|B)$，我们假设所有的特征相互独立，则我们就有： 2
$$ P(A_1 A_2 &mldr; A_n|B)=\prod_{i=1}^{n}{P(A_i|B)} \tag{3} $$
其中 $P(A_i|B)$ 是容易从数据集中直接得到的，分为离散和连续两种情况：
 当 $A_i$ 为离散型随机变量时  $$ P(A_i|B)=\frac{B类中有特征A_i的数据个数}{B类数据个数} $$"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#262d33"><title>被门夹过的核桃还补脑吗 - 朴素贝叶斯分类器</title><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=/minima.1657770635.css><script defer type=text/javascript src=/minima.1657770635.js></script></head><script>let theme_2b_used=window.matchMedia('(prefers-color-scheme: dark)').matches?'dark':'light';try{if(!('theme'in localStorage)){const a='system';(a==='dark'||a==='light')&&(theme_2b_used=a),localStorage.theme=theme_2b_used}document.querySelector('html').classList.add(localStorage.theme)}catch(a){console.error(a)}</script><body class="sm:mx-5 sm:my-0"><header class="flex justify-between items-center mb-6 sm:my-3"><div class="flex items-center"><div id=theme-switcher class="text-4xl cursor-pointer">🌝</div></div><nav class="flex items-center
whitespace-nowrap overflow-x-auto overflow-y-hidden"><a class=ml-5 href=/>首页</a>
<a class=ml-5 href=/tech>技术</a>
<a class=ml-5 href=/idea>想法</a>
<a class=ml-5 href=/read>阅读</a>
<a class=ml-5 href=/about>关于</a></nav></header><details class="toc toc-lines"><summary></summary><div class=pb-1><nav id=TableOfContents><ul><li><a href=#基本原理>基本原理</a></li><li><a href=#应用示例>应用示例</a></li><li><a href=#拉普拉斯修正>拉普拉斯修正</a></li></ul></nav></div></details><h1 class="mt-6 mb-6">朴素贝叶斯分类器</h1><div class="mb-3 text-xs flex justify-between sm:flex-col"><div>Posted at &mdash; Nov 10, 2019</div></div><main><p></p><article class=md><p>这篇文章将对朴素贝叶斯分类器进行简单描述和公式推导，并以性别预测为例来理解其「朴素」的原因，最后你应该能根据文章的描述编写一个简单的由人的姓名预测其性别的小程序。阅读前你可能需要懂一些概率论的知识，比如条件概率、贝叶斯公式等等。</p><h2 id=基本原理>基本原理</h2><p>我们在小学二年级学过贝叶斯公式：</p><p>$$
P(B|A)=\frac{P(B)P(A|B)}{P(A)} \tag{1}
$$</p><p>通过该式子，我们可以算出在已知事件A发生的条件下，事件B发生的概率。比如<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>当我们看到室友抽屉里藏女士内衣，则室友是个变态的概率就是<em>遇到变态室友的概率乘室友是个变态还喜欢把内衣放抽屉里的概率再除以室友抽屉里有内衣的概率</em>。这里面其实蕴含「后验概率」的思想。感兴趣的可以 <a href=https://simple.wikipedia.org/wiki/Bayes%27_theorem>了解下</a>。</p><p>现在我们假设上式中的事件A由很多事件构成，即：</p><p>$$
A=A_1 \cap A_2 \cap &mldr; \cap A_n
$$</p><p>则我们的贝叶斯公式变成：</p><p>$$
P(B|A_1 A_2 &mldr; A_n)=\frac{P(B)P(A_1 A_2 &mldr; A_n|B)}{P(A_1 A_2 &mldr; A_n)} \tag{2}
$$</p><p>这其实已经是一个分类器的模型了。我们不妨将事件B看成一推数据的分类结果，将 $A_1,A_2,&mldr;,A_n$ 看成是导致该结果的因素或者叫特征（以下统称特征），则等号左边表示的就是我们的数据属于分类 $B$ 的概率，当其达到某个阈值，我们可以认为该推数据就属于某个分类。</p><p>要得到等号左边是多少，我们得从已有的数据中找到等号右边的三项的值。其中 $P(B)$ 可以通过统计数据集中 $B$ 类样本出现的频次得到，即：</p><p>$$
P(B)=\frac{B类样本个数}{样本总个数}
$$</p><p>但剩下的两项很难从有限的数据中直接得出。对于 $P(A_1 A_2 &mldr; A_n|B)$，我们假设<strong>所有的特征相互独立</strong>，则我们就有： <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><p>$$
P(A_1 A_2 &mldr; A_n|B)=\prod_{i=1}^{n}{P(A_i|B)} \tag{3}
$$</p><p>其中 $P(A_i|B)$ 是容易从数据集中直接得到的，分为离散和连续两种情况：</p><ul><li>当 $A_i$ 为离散型随机变量时</li></ul><p>$$
P(A_i|B)=\frac{B类中有特征A_i的数据个数}{B类数据个数}
$$</p><ul><li>当 $A_i$ 为连续型随机变量时</li></ul><p>假设 $A_i|B$ 符合期望为 $\mu_{ki}$，方差为 $\sigma^2_{ki}$ 的正态分布，则有：</p><p>$$
P(A_i|B)=\frac{1}{\sqrt{2\pi}\sigma_{ki}}e^{{-\frac{(A_i-\mu_{ki})^2}{2\sigma^2_{ki}}}}
$$</p><p>而对于2式等号右边分母部分，我们通过一个技巧来绕开对它的依赖。思考一下，我们的分类模型给我们的结果 $P(B|A_1 A_2 &mldr; A_n)$ 是一个有特征 $A_1 A_2 &mldr; A_n$ 的对象属于某一类的可能性大小，并不是一个确定的分类结果，我们可以算出该对象属于第 $k$ 类的概率：$P(B_k|A_1 A_2 &mldr; A_n)$，并对所有的 $P(B_k|A_1 A_2 &mldr; A_n)$ 进行比较，将最大的那个，即可能性最大的分类结果作为最终的分类结果。而对每个 $P(B_k|A_i)$ 的计算，其分母 $P(A_1 A_2 &mldr; A_n)$ 都是相同的，故可以不将该分母代入计算，那么我们最终的分类器模型就变成了：</p><p>$$
\max{P(B_k|A_1 A_2 &mldr; A_n)}=\max{P(B_k)\prod_{i=1}^{n}{P(A_i|B_k)}}
$$</p><h2 id=应用示例>应用示例</h2><p>现在假设我们有1万人的姓名-性别数据，其中4500人为女生，5500人为男生，且名字里有「徐」字的男女生人数分别为10、20人，名字里有「坤」字的男女生人数为80、10人。将「名字」看成是不同的特征，「性别」看成是分类结果，于是就可以利用上式算出名字是「徐坤」的人是男生的概率为：</p><p>同理得到名字是「徐坤」的人是男生的概率为：</p><p>我们发现算出来的两个结果都非常的小，因为我们提供的数据的特征分布比较稀疏，计算出的结果很可能小到让计算机发生位溢出，为避免这种情况，对于二分问题，我们可以在计算前先做商，即：</p><p>$$
r=\frac{P(B_1|A_1 A_2 &mldr; A_n)}{P(B_2|A_1 A_2 &mldr; A_n)}
$$</p><p>这样就都转换到整型数据的计算上了，对此我们可以合理地认为当 $r>1$ 时，分类到第一类，当 $r&lt;1$ 时分类到第二类。对于上述例子：</p><p>$$
r=\biggl(\frac{5500}{10000}\times\frac{10}{5500}\times\frac{80}{5500}\biggr)\div\biggl(\frac{4500}{10000}\times\frac{20}{4500}\times\frac{10}{4500}\biggr) \approx 3.27 > 1
$$</p><p>故名字为「徐坤」的人被分类到男生中。</p><p>当然我们不一定要用 1 作为分类的界限，选择多少作为该界限可以通过跑测试数据集查看拟合程度，若发现大量男生被分类到女生，就降低界限，反之亦然。</p><h2 id=拉普拉斯修正>拉普拉斯修正</h2><p>细心的你一定发现了一个问题，如果我们的数据中没有名字里有坤字的男生的话，会导致 $r$ 分子为零，又或者没有名字里有坤字的女生，导致 $r$ 的分母为零，我们的模型就失效了。为避免这种极端的情况发生，我们可以在统计名字里有坤字的人个数时人为地加一来避免零的出现，对于我们的分类模型，就是：</p><p>$$
P(A_i|B_k)=\frac{第k类中有特征A_i的样本个数+1}{第k类样本个数+不同的特征个数}
$$</p><p>其中「不同的特征个数」，在我们的例子中，就是指数据中不同的字的个数。这种对统计数据的处理就叫做拉普拉斯修正。</p><p>需要注意的是，若在计算中使用了拉普拉斯修正，则同时需要对先验概率进行修正，即</p><p>$$
P(B_k)=\frac{第k类样本个数+1}{样本总数+类别数}
$$</p><p>对于二分类问题，「类别数」就为 2。</p><p>最后，关于对该性别预测例子的实现，其实有人早在几年前做了（见<a href=https://github.com/observerss/ngender>GitHub</a>），并且公开了数据集（2000w条），可以拿来试试。该作者对最后一步概率的处理和博主描述的有所不同，从源码中可以看出，是将男女生的计算结果分别取了在其和的占比，间接地算出了是男、女生的概率，也是个不错的方法。</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>该例子取自<a href=https://www.bilibili.com/video/BV1wf4y1d7iU>毕导 - 我是如何判断室友是不是变态的？这要从贝叶斯概率说起</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>该特征相互独立的假设就是朴素贝叶斯分类器「朴素」的地方。&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article></main><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css integrity=sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js integrity=sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:!0},{left:'$',right:'$',display:!1},{left:'\\(',right:'\\)',display:!1},{left:'\\[',right:'\\]',display:!0}],throwOnError:!1})})</script><div id=ovo_thread class=my-8><div class="flex flex-col items-center">评论插件加载中 OvO</div></div><link rel=stylesheet href=//unpkg.com/@ovojs/ovo/dist/style.css><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;const a=document.createElement('script');a.type='text/javascript',a.src='//unpkg.com/@ovojs/ovo',(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(a),a.addEventListener('load',function(){const a=document.getElementById('ovo_thread');a.innerHTML='',new OvO({target:a,props:{server:"https://ovo.nmslwsnd.com",placeholder:""}})})})()</script><footer class="mt-8 flex sm:flex-col-reverse justify-between items-center"><p class="mt-0 text-sm">© 2018-2022 XJJ |
<a href=https://gohugo.io target=_blank rel="noopener noreferrer">Hugo</a> on
<a href=https://github.com/mivinci/hugo-theme-minima target=_blank rel="noopener noreferrer">Minima</a></p><p class="flex items-center mt-0"><a class="icon mx-2" href=https://github.com/mivinci title=github><svg fill="#63636f" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63.0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577.0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93.0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176.0.0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22.0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22.0 1.606-.015 2.896-.015 3.286.0.315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="icon mx-2" href=https://linkedin/in/leonard-mivinci-63895317a title=linkedin><svg fill="#0073b1" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853.0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601.0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144.0-2.063-.926-2.063-2.065.0-1.138.92-2.063 2.063-2.063 1.14.0 2.064.925 2.064 2.063.0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225.0H1.771C.792.0.0.774.0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2.0 22.222.0h.003z"/></svg></a><a class="icon mx-2" href=https://twitter.com/Leonard14733745 title=twitter><svg fill="#1da1f2" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Twitter</title><path d="M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717.0-4.92 2.203-4.92 4.917.0.39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475.0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314.0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39.0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054.0 13.999-7.496 13.999-13.986.0-.209.0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z"/></svg></a><a class="icon mx-2" href=/index.xml title=rss><svg fill="#63636f" t="1626591563876" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="1984" width="18" height="16"><path d="M128 768a128 128 0 100 256 128 128 0 000-256zM0 368v176c265.104.0 480 214.912 480 480h176c0-362.32-293.696-656-656-656zM0 0v176c468.336.0 848 379.664 848 848h176C1024 458.464 565.536.0.0.0z" p-id="1985"/></svg></a></p></footer></body></html>